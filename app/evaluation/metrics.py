"""
Robustness Metrics Module

Provides functions to calculate adversarial robustness metrics.
"""

import torch
import torch.nn as nn
from typing import Dict, Tuple
import logging

logger = logging.getLogger(__name__)


def calculate_clean_accuracy(
    model: nn.Module,
    dataloader,
    device: str = "cpu"
) -> float:
    """
    Calculate clean accuracy (no adversarial perturbations).
    
    Args:
        model: PyTorch model to evaluate
        dataloader: DataLoader with test data
        device: Device to run on
    
    Returns:
        Clean accuracy as a percentage (0-100)
    """
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = 100.0 * correct / total if total > 0 else 0.0
    return accuracy


def calculate_robust_accuracy(
    model: nn.Module,
    dataloader,
    attack_fn,
    epsilon: float,
    device: str = "cpu",
    **attack_kwargs
) -> Tuple[float, float, float]:
    """
    Calculate robust accuracy under adversarial attack.
    
    This function evaluates the model on adversarial examples generated by
    the specified attack function.
    
    Args:
        model: PyTorch model to evaluate
        dataloader: DataLoader with test data
        attack_fn: Attack function (e.g., fgsm_attack, pgd_attack)
        epsilon: Perturbation budget
        device: Device to run on
        **attack_kwargs: Additional arguments for the attack function
    
    Returns:
        Tuple of (clean_accuracy, robust_accuracy, attack_success_rate)
        All values are percentages (0-100)
    """
    model.eval()
    
    clean_correct = 0
    adv_correct = 0
    total = 0
    attack_successful_count = 0
    
    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)
        
        # Clean predictions
        with torch.no_grad():
            outputs_clean = model(images)
            _, predicted_clean = torch.max(outputs_clean.data, 1)
            clean_correct += (predicted_clean == labels).sum().item()
        
        # Generate adversarial examples
        adv_images = attack_fn(
            model,
            images,
            labels,
            epsilon=epsilon,
            device=device,
            **attack_kwargs
        )
        
        # Adversarial predictions
        with torch.no_grad():
            outputs_adv = model(adv_images)
            _, predicted_adv = torch.max(outputs_adv.data, 1)
            adv_correct += (predicted_adv == labels).sum().item()
        
        # Count successful attacks (correctly classified clean → misclassified adv)
        attack_successful_count += (
            (predicted_clean == labels) & (predicted_adv != labels)
        ).sum().item()
        
        total += labels.size(0)
    
    # Calculate metrics
    clean_accuracy = 100.0 * clean_correct / total if total > 0 else 0.0
    robust_accuracy = 100.0 * adv_correct / total if total > 0 else 0.0
    attack_success_rate = (
        100.0 * attack_successful_count / clean_correct
        if clean_correct > 0 else 0.0
    )
    
    return clean_accuracy, robust_accuracy, attack_success_rate


def evaluate_model_robustness(
    model: nn.Module,
    dataloader,
    attack_fn,
    epsilon: float,
    attack_name: str,
    device: str = "cpu",
    **attack_kwargs
) -> Dict[str, float]:
    """
    Comprehensive robustness evaluation returning all metrics.
    
    Args:
        model: PyTorch model to evaluate
        dataloader: DataLoader with test data
        attack_fn: Attack function
        epsilon: Perturbation budget
        attack_name: Name of the attack (for logging)
        device: Device to run on
        **attack_kwargs: Additional attack parameters
    
    Returns:
        Dictionary with metrics:
            - clean_accuracy: Accuracy on clean examples (%)
            - robust_accuracy: Accuracy on adversarial examples (%)
            - attack_success_rate: Rate of successful attacks (%)
            - total_samples: Number of samples evaluated
    """
    logger.info(f"Running {attack_name} attack with epsilon={epsilon}")
    
    # Calculate all metrics
    clean_acc, robust_acc, asr = calculate_robust_accuracy(
        model,
        dataloader,
        attack_fn,
        epsilon,
        device,
        **attack_kwargs
    )
    
    # Count total samples
    total_samples = sum(labels.size(0) for _, labels in dataloader)
    
    results = {
        "clean_accuracy": round(clean_acc, 2),
        "robust_accuracy": round(robust_acc, 2),
        "attack_success_rate": round(asr, 2),
        "total_samples": total_samples
    }
    
    logger.info(
        f"{attack_name} (ε={epsilon}): "
        f"Clean={results['clean_accuracy']}%, "
        f"Robust={results['robust_accuracy']}%, "
        f"ASR={results['attack_success_rate']}%"
    )
    
    return results
